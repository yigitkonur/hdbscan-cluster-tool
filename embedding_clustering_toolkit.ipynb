{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¨ Embedding Clustering Toolkit\n",
    "\n",
    "**A comprehensive toolkit for clustering high-dimensional text embeddings using DBSCAN and HDBSCAN algorithms.**\n",
    "\n",
    "This notebook provides an interactive, configurable workflow for:\n",
    "- Loading and preprocessing embedding vectors from CSV files\n",
    "- Finding optimal clustering parameters automatically\n",
    "- Performing DBSCAN clustering with cosine similarity\n",
    "- Performing HDBSCAN clustering with PCA dimensionality reduction\n",
    "- Visualizing and exporting results\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup & Configuration](#1-setup--configuration)\n",
    "2. [Data Loading](#2-data-loading)\n",
    "3. [Parameter Search (Sweet Spot Finder)](#3-parameter-search-sweet-spot-finder)\n",
    "4. [DBSCAN Clustering](#4-dbscan-clustering)\n",
    "5. [HDBSCAN with PCA](#5-hdbscan-with-pca)\n",
    "6. [Results Export](#6-results-export)\n",
    "7. [Visualization](#7-visualization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Configuration\n",
    "\n",
    "Install required packages and configure your analysis parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment if needed)\n",
    "# !pip install pandas numpy scikit-learn hdbscan openpyxl matplotlib seaborn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Tuple, Optional, List, Dict, Any\n",
    "from dataclasses import dataclass\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "\n",
    "try:\n",
    "    import hdbscan\n",
    "    HDBSCAN_AVAILABLE = True\n",
    "except ImportError:\n",
    "    HDBSCAN_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è hdbscan not installed. Install with: pip install hdbscan\")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "print(f\"   - HDBSCAN available: {HDBSCAN_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# üìã CONFIGURATION - Modify these parameters for your use case\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class ClusteringConfig:\n",
    "    \"\"\"Configuration for embedding clustering analysis.\"\"\"\n",
    "    \n",
    "    # File paths\n",
    "    input_csv_path: str = \"sample.csv\"  # Path to your CSV file with embeddings\n",
    "    output_xlsx_path: str = \"clustering_results.xlsx\"  # Output file path\n",
    "    \n",
    "    # Vector configuration\n",
    "    vector_dimension: int = 3072  # Expected dimension (3072 for text-embedding-3-large)\n",
    "    vector_columns: List[str] = None  # Column names containing vector parts (auto-detect if None)\n",
    "    name_column: str = \"Name\"  # Column containing entity names/labels\n",
    "    \n",
    "    # DBSCAN parameters\n",
    "    similarity_threshold: float = 0.78  # Cosine similarity threshold (higher = tighter clusters)\n",
    "    min_samples: int = 2  # Minimum samples to form a cluster\n",
    "    \n",
    "    # HDBSCAN + PCA parameters  \n",
    "    n_pca_components: int = 30  # PCA dimensions for HDBSCAN\n",
    "    hdbscan_metric: str = \"euclidean\"  # Distance metric for HDBSCAN\n",
    "    hdbscan_min_cluster_size: int = 2  # Minimum cluster size for HDBSCAN\n",
    "    hdbscan_min_samples: int = 1  # Min samples for HDBSCAN\n",
    "    \n",
    "    # Parameter search ranges\n",
    "    threshold_range: Tuple[float, float, float] = (0.995, 0.800, -0.005)  # (start, stop, step)\n",
    "    min_samples_range: List[int] = None  # List of min_samples to test\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.vector_columns is None:\n",
    "            self.vector_columns = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\"]\n",
    "        if self.min_samples_range is None:\n",
    "            self.min_samples_range = [2, 3, 5]\n",
    "\n",
    "# Initialize config - MODIFY THIS FOR YOUR DATA\n",
    "config = ClusteringConfig(\n",
    "    input_csv_path=\"sample.csv\",\n",
    "    output_xlsx_path=\"clustering_results.xlsx\",\n",
    "    vector_dimension=3072,\n",
    "    similarity_threshold=0.78,\n",
    "    min_samples=2,\n",
    ")\n",
    "\n",
    "print(\"üìã Configuration loaded:\")\n",
    "print(f\"   - Input: {config.input_csv_path}\")\n",
    "print(f\"   - Vector dimension: {config.vector_dimension}\")\n",
    "print(f\"   - Similarity threshold: {config.similarity_threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Data Loading\n",
    "\n",
    "Load your CSV file containing embedding vectors. This handles:\n",
    "- Multiple column formats (split vectors or single column)\n",
    "- Automatic validation of vector dimensions\n",
    "- Filtering invalid/incomplete vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDataLoader:\n",
    "    \"\"\"Load and preprocess embedding vectors from CSV files.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: ClusteringConfig):\n",
    "        self.config = config\n",
    "        self.df: Optional[pd.DataFrame] = None\n",
    "        self.valid_vectors_df: Optional[pd.DataFrame] = None\n",
    "        self.vector_matrix: Optional[np.ndarray] = None\n",
    "    \n",
    "    def load(self, file_path: Optional[str] = None) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Load data from CSV and prepare for clustering.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (original_df, valid_vectors_df)\n",
    "        \"\"\"\n",
    "        file_path = file_path or self.config.input_csv_path\n",
    "        \n",
    "        print(f\"üìÇ Loading data from: {file_path}\")\n",
    "        self.df = pd.read_csv(file_path)\n",
    "        print(f\"   - Total rows: {len(self.df):,}\")\n",
    "        print(f\"   - Columns: {list(self.df.columns)}\")\n",
    "        \n",
    "        # Parse vectors from columns\n",
    "        self._parse_vectors()\n",
    "        \n",
    "        # Filter valid vectors\n",
    "        self._filter_valid_vectors()\n",
    "        \n",
    "        return self.df, self.valid_vectors_df\n",
    "    \n",
    "    def _parse_vectors(self) -> None:\n",
    "        \"\"\"Parse vector columns into numpy arrays.\"\"\"\n",
    "        \n",
    "        # Check if vectors are split across multiple columns\n",
    "        if all(col in self.df.columns for col in self.config.vector_columns):\n",
    "            print(f\"   - Parsing vectors from columns: {self.config.vector_columns}\")\n",
    "            self.df['vector_array'] = self.df.apply(\n",
    "                lambda row: self._concat_vector_parts(row, self.config.vector_columns),\n",
    "                axis=1\n",
    "            )\n",
    "        # Check for single 'embedding' or 'vector' column\n",
    "        elif 'embedding' in self.df.columns:\n",
    "            print(\"   - Parsing vectors from 'embedding' column\")\n",
    "            self.df['vector_array'] = self.df['embedding'].apply(self._parse_single_column)\n",
    "        elif 'vector' in self.df.columns:\n",
    "            print(\"   - Parsing vectors from 'vector' column\")\n",
    "            self.df['vector_array'] = self.df['vector'].apply(self._parse_single_column)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Could not find vector columns. Expected one of:\\n\"\n",
    "                f\"  - Split columns: {self.config.vector_columns}\\n\"\n",
    "                f\"  - Single column: 'embedding' or 'vector'\"\n",
    "            )\n",
    "        \n",
    "        # Initialize cluster column\n",
    "        self.df['cluster'] = -1\n",
    "    \n",
    "    def _concat_vector_parts(self, row: pd.Series, columns: List[str]) -> np.ndarray:\n",
    "        \"\"\"Concatenate vector parts from multiple columns.\"\"\"\n",
    "        parts = []\n",
    "        for col in columns:\n",
    "            val = str(row[col])\n",
    "            if val and val != 'nan':\n",
    "                parts.extend([\n",
    "                    float(x.strip()) \n",
    "                    for x in val.split(',') \n",
    "                    if x.strip()\n",
    "                ])\n",
    "        return np.array(parts)\n",
    "    \n",
    "    def _parse_single_column(self, value: str) -> np.ndarray:\n",
    "        \"\"\"Parse vector from a single column (comma or JSON format).\"\"\"\n",
    "        if pd.isna(value):\n",
    "            return np.array([])\n",
    "        \n",
    "        value = str(value).strip()\n",
    "        \n",
    "        # Handle JSON array format\n",
    "        if value.startswith('['):\n",
    "            import json\n",
    "            return np.array(json.loads(value))\n",
    "        \n",
    "        # Handle comma-separated format\n",
    "        return np.array([float(x.strip()) for x in value.split(',') if x.strip()])\n",
    "    \n",
    "    def _filter_valid_vectors(self) -> None:\n",
    "        \"\"\"Filter rows with valid vector dimensions.\"\"\"\n",
    "        vector_lengths = self.df['vector_array'].apply(len)\n",
    "        \n",
    "        # Show dimension distribution\n",
    "        print(f\"\\nüìä Vector dimension distribution:\")\n",
    "        for dim, count in vector_lengths.value_counts().head(5).items():\n",
    "            status = \"‚úÖ\" if dim == self.config.vector_dimension else \"‚ùå\"\n",
    "            print(f\"   {status} Dimension {dim}: {count:,} vectors\")\n",
    "        \n",
    "        # Filter valid vectors\n",
    "        mask = vector_lengths == self.config.vector_dimension\n",
    "        self.valid_vectors_df = self.df[mask].copy()\n",
    "        \n",
    "        invalid_count = (~mask).sum()\n",
    "        if invalid_count > 0:\n",
    "            print(f\"\\n‚ö†Ô∏è Filtered out {invalid_count:,} rows with invalid dimensions\")\n",
    "        \n",
    "        print(f\"‚úÖ Valid vectors: {len(self.valid_vectors_df):,}\")\n",
    "        \n",
    "        # Create vector matrix for efficient computation\n",
    "        self.vector_matrix = np.stack(self.valid_vectors_df['vector_array'].values)\n",
    "    \n",
    "    def get_vector_matrix(self) -> np.ndarray:\n",
    "        \"\"\"Get the stacked vector matrix for clustering.\"\"\"\n",
    "        if self.vector_matrix is None:\n",
    "            raise ValueError(\"Data not loaded. Call load() first.\")\n",
    "        return self.vector_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "loader = EmbeddingDataLoader(config)\n",
    "df, valid_df = loader.load()\n",
    "\n",
    "print(f\"\\nüìã Data preview:\")\n",
    "display(valid_df[[config.name_column]].head(10) if config.name_column in valid_df.columns else valid_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Parameter Search (Sweet Spot Finder)\n",
    "\n",
    "Find optimal DBSCAN parameters by testing different similarity thresholds and min_samples values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParameterSearcher:\n",
    "    \"\"\"Find optimal clustering parameters through grid search.\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_matrix: np.ndarray):\n",
    "        self.vector_matrix = vector_matrix\n",
    "        self.results: List[Dict[str, Any]] = []\n",
    "    \n",
    "    def search(\n",
    "        self,\n",
    "        threshold_range: Tuple[float, float, float] = (0.995, 0.800, -0.005),\n",
    "        min_samples_list: List[int] = [2, 3, 5],\n",
    "        show_progress: bool = True\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Search for optimal DBSCAN parameters.\n",
    "        \n",
    "        Args:\n",
    "            threshold_range: (start, stop, step) for similarity thresholds\n",
    "            min_samples_list: List of min_samples values to test\n",
    "            show_progress: Show progress bar\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with results for each parameter combination\n",
    "        \"\"\"\n",
    "        self.results = []\n",
    "        thresholds = np.arange(*threshold_range)\n",
    "        \n",
    "        total = len(thresholds) * len(min_samples_list)\n",
    "        iterator = tqdm(total=total, desc=\"üîç Parameter search\") if show_progress else None\n",
    "        \n",
    "        for min_samples in min_samples_list:\n",
    "            for threshold in thresholds:\n",
    "                result = self._evaluate_params(threshold, min_samples)\n",
    "                self.results.append(result)\n",
    "                \n",
    "                if iterator:\n",
    "                    iterator.update(1)\n",
    "        \n",
    "        if iterator:\n",
    "            iterator.close()\n",
    "        \n",
    "        return self._create_results_df()\n",
    "    \n",
    "    def _evaluate_params(\n",
    "        self, \n",
    "        similarity_threshold: float, \n",
    "        min_samples: int\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Evaluate a single parameter combination.\"\"\"\n",
    "        eps_value = 1 - similarity_threshold\n",
    "        \n",
    "        db = DBSCAN(eps=eps_value, min_samples=min_samples, metric='cosine')\n",
    "        labels = db.fit_predict(self.vector_matrix)\n",
    "        \n",
    "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "        n_noise = (labels == -1).sum()\n",
    "        n_clustered = len(labels) - n_noise\n",
    "        \n",
    "        # Calculate silhouette score if possible\n",
    "        silhouette = np.nan\n",
    "        if 1 < n_clusters < len(labels) and n_clustered > n_clusters:\n",
    "            try:\n",
    "                silhouette = silhouette_score(self.vector_matrix, labels, metric='cosine')\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return {\n",
    "            'similarity_threshold': similarity_threshold,\n",
    "            'min_samples': min_samples,\n",
    "            'n_clusters': n_clusters,\n",
    "            'n_noise': n_noise,\n",
    "            'noise_ratio': n_noise / len(labels),\n",
    "            'clustered_ratio': n_clustered / len(labels),\n",
    "            'avg_cluster_size': n_clustered / n_clusters if n_clusters > 0 else 0,\n",
    "            'silhouette_score': silhouette\n",
    "        }\n",
    "    \n",
    "    def _create_results_df(self) -> pd.DataFrame:\n",
    "        \"\"\"Create results DataFrame sorted by quality.\"\"\"\n",
    "        results_df = pd.DataFrame(self.results)\n",
    "        \n",
    "        # Sort by silhouette score (higher is better), then by cluster count\n",
    "        results_df = results_df.sort_values(\n",
    "            by=['silhouette_score', 'n_clusters'],\n",
    "            ascending=[False, False]\n",
    "        )\n",
    "        \n",
    "        return results_df\n",
    "    \n",
    "    def plot_results(self, results_df: pd.DataFrame) -> None:\n",
    "        \"\"\"Visualize parameter search results.\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        \n",
    "        for idx, min_samples in enumerate(results_df['min_samples'].unique()):\n",
    "            subset = results_df[results_df['min_samples'] == min_samples]\n",
    "            \n",
    "            # Plot 1: Clusters vs Threshold\n",
    "            axes[0, 0].plot(\n",
    "                subset['similarity_threshold'], \n",
    "                subset['n_clusters'],\n",
    "                marker='o',\n",
    "                label=f'min_samples={min_samples}'\n",
    "            )\n",
    "            \n",
    "            # Plot 2: Noise Ratio vs Threshold\n",
    "            axes[0, 1].plot(\n",
    "                subset['similarity_threshold'],\n",
    "                subset['noise_ratio'],\n",
    "                marker='o',\n",
    "                label=f'min_samples={min_samples}'\n",
    "            )\n",
    "            \n",
    "            # Plot 3: Silhouette Score vs Threshold\n",
    "            axes[1, 0].plot(\n",
    "                subset['similarity_threshold'],\n",
    "                subset['silhouette_score'],\n",
    "                marker='o',\n",
    "                label=f'min_samples={min_samples}'\n",
    "            )\n",
    "            \n",
    "            # Plot 4: Avg Cluster Size vs Threshold\n",
    "            axes[1, 1].plot(\n",
    "                subset['similarity_threshold'],\n",
    "                subset['avg_cluster_size'],\n",
    "                marker='o',\n",
    "                label=f'min_samples={min_samples}'\n",
    "            )\n",
    "        \n",
    "        axes[0, 0].set(xlabel='Similarity Threshold', ylabel='Number of Clusters', title='Clusters vs Threshold')\n",
    "        axes[0, 1].set(xlabel='Similarity Threshold', ylabel='Noise Ratio', title='Noise vs Threshold')\n",
    "        axes[1, 0].set(xlabel='Similarity Threshold', ylabel='Silhouette Score', title='Quality vs Threshold')\n",
    "        axes[1, 1].set(xlabel='Similarity Threshold', ylabel='Avg Cluster Size', title='Cluster Size vs Threshold')\n",
    "        \n",
    "        for ax in axes.flat:\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.suptitle('DBSCAN Parameter Search Results', y=1.02, fontsize=14, fontweight='bold')\n",
    "        plt.show()\n",
    "    \n",
    "    def get_best_params(self, results_df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"Get the best parameter combination.\"\"\"\n",
    "        # Filter for valid silhouette scores\n",
    "        valid_results = results_df[results_df['silhouette_score'].notna()]\n",
    "        \n",
    "        if len(valid_results) == 0:\n",
    "            print(\"‚ö†Ô∏è No valid silhouette scores. Using highest cluster count.\")\n",
    "            best = results_df.iloc[results_df['n_clusters'].idxmax()]\n",
    "        else:\n",
    "            best = valid_results.iloc[0]  # Already sorted by silhouette\n",
    "        \n",
    "        return best.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run parameter search\n",
    "searcher = ParameterSearcher(loader.get_vector_matrix())\n",
    "\n",
    "# Customize search range if needed\n",
    "search_results = searcher.search(\n",
    "    threshold_range=(0.95, 0.70, -0.01),  # Coarser search for speed\n",
    "    min_samples_list=[2, 3, 5],\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "print(\"\\nüìä Top 10 Parameter Combinations:\")\n",
    "display(search_results.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize parameter search results\n",
    "searcher.plot_results(search_results)\n",
    "\n",
    "# Get best parameters\n",
    "best_params = searcher.get_best_params(search_results)\n",
    "print(\"\\nüéØ Best Parameters Found:\")\n",
    "for key, value in best_params.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"   - {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"   - {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. DBSCAN Clustering\n",
    "\n",
    "Perform DBSCAN clustering using cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBSCANClusterer:\n",
    "    \"\"\"DBSCAN clustering for high-dimensional embeddings using cosine similarity.\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_matrix: np.ndarray):\n",
    "        self.vector_matrix = vector_matrix\n",
    "        self.labels: Optional[np.ndarray] = None\n",
    "        self.stats: Dict[str, Any] = {}\n",
    "    \n",
    "    def fit(\n",
    "        self,\n",
    "        similarity_threshold: float = 0.78,\n",
    "        min_samples: int = 2\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Perform DBSCAN clustering.\n",
    "        \n",
    "        Args:\n",
    "            similarity_threshold: Cosine similarity threshold (0-1)\n",
    "            min_samples: Minimum samples for core points\n",
    "            \n",
    "        Returns:\n",
    "            Array of cluster labels (-1 = noise)\n",
    "        \"\"\"\n",
    "        print(f\"\\nüîß Running DBSCAN clustering...\")\n",
    "        print(f\"   - Similarity threshold: {similarity_threshold}\")\n",
    "        print(f\"   - Min samples: {min_samples}\")\n",
    "        \n",
    "        eps_value = 1 - similarity_threshold\n",
    "        \n",
    "        db = DBSCAN(eps=eps_value, min_samples=min_samples, metric='cosine')\n",
    "        self.labels = db.fit_predict(self.vector_matrix)\n",
    "        \n",
    "        self._calculate_stats()\n",
    "        self._print_summary()\n",
    "        \n",
    "        return self.labels\n",
    "    \n",
    "    def _calculate_stats(self) -> None:\n",
    "        \"\"\"Calculate clustering statistics.\"\"\"\n",
    "        labels = self.labels\n",
    "        \n",
    "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "        n_noise = (labels == -1).sum()\n",
    "        n_clustered = len(labels) - n_noise\n",
    "        \n",
    "        # Cluster size distribution\n",
    "        cluster_sizes = pd.Series(labels).value_counts().sort_index()\n",
    "        if -1 in cluster_sizes.index:\n",
    "            cluster_sizes = cluster_sizes.drop(-1)\n",
    "        \n",
    "        # Silhouette score\n",
    "        silhouette = np.nan\n",
    "        if 1 < n_clusters < len(labels):\n",
    "            try:\n",
    "                silhouette = silhouette_score(self.vector_matrix, labels, metric='cosine')\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        self.stats = {\n",
    "            'n_clusters': n_clusters,\n",
    "            'n_noise': n_noise,\n",
    "            'n_clustered': n_clustered,\n",
    "            'noise_ratio': n_noise / len(labels),\n",
    "            'clustered_ratio': n_clustered / len(labels),\n",
    "            'avg_cluster_size': n_clustered / n_clusters if n_clusters > 0 else 0,\n",
    "            'cluster_sizes': cluster_sizes.to_dict(),\n",
    "            'silhouette_score': silhouette\n",
    "        }\n",
    "    \n",
    "    def _print_summary(self) -> None:\n",
    "        \"\"\"Print clustering summary.\"\"\"\n",
    "        s = self.stats\n",
    "        \n",
    "        print(f\"\\nüìä DBSCAN Results:\")\n",
    "        print(f\"   ‚îú‚îÄ Clusters: {s['n_clusters']}\")\n",
    "        print(f\"   ‚îú‚îÄ Noise points: {s['n_noise']} ({s['noise_ratio']:.1%})\")\n",
    "        print(f\"   ‚îú‚îÄ Clustered points: {s['n_clustered']} ({s['clustered_ratio']:.1%})\")\n",
    "        print(f\"   ‚îú‚îÄ Avg cluster size: {s['avg_cluster_size']:.1f}\")\n",
    "        if not np.isnan(s['silhouette_score']):\n",
    "            print(f\"   ‚îî‚îÄ Silhouette score: {s['silhouette_score']:.3f}\")\n",
    "        else:\n",
    "            print(f\"   ‚îî‚îÄ Silhouette score: N/A\")\n",
    "    \n",
    "    def get_cluster_summary(self) -> pd.DataFrame:\n",
    "        \"\"\"Get summary DataFrame of cluster sizes.\"\"\"\n",
    "        if self.labels is None:\n",
    "            raise ValueError(\"No clustering performed yet. Call fit() first.\")\n",
    "        \n",
    "        sizes = pd.Series(self.labels).value_counts().sort_values(ascending=False)\n",
    "        summary = pd.DataFrame({\n",
    "            'cluster_id': sizes.index,\n",
    "            'size': sizes.values,\n",
    "            'percentage': (sizes.values / len(self.labels) * 100).round(2)\n",
    "        })\n",
    "        summary['type'] = summary['cluster_id'].apply(lambda x: 'Noise' if x == -1 else 'Cluster')\n",
    "        \n",
    "        return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run DBSCAN with optimal or custom parameters\n",
    "dbscan_clusterer = DBSCANClusterer(loader.get_vector_matrix())\n",
    "\n",
    "# Option 1: Use best parameters from search\n",
    "# dbscan_labels = dbscan_clusterer.fit(\n",
    "#     similarity_threshold=best_params['similarity_threshold'],\n",
    "#     min_samples=int(best_params['min_samples'])\n",
    "# )\n",
    "\n",
    "# Option 2: Use custom parameters\n",
    "dbscan_labels = dbscan_clusterer.fit(\n",
    "    similarity_threshold=config.similarity_threshold,\n",
    "    min_samples=config.min_samples\n",
    ")\n",
    "\n",
    "# Show cluster summary\n",
    "print(\"\\nüìã Cluster Size Distribution:\")\n",
    "display(dbscan_clusterer.get_cluster_summary().head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. HDBSCAN with PCA\n",
    "\n",
    "Use PCA for dimensionality reduction followed by HDBSCAN clustering. This approach:\n",
    "- Reduces computation time significantly\n",
    "- Can improve clustering quality for very high-dimensional data\n",
    "- Enables visualization in 2D/3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDBSCANClusterer:\n",
    "    \"\"\"HDBSCAN clustering with optional PCA dimensionality reduction.\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_matrix: np.ndarray):\n",
    "        self.vector_matrix = vector_matrix\n",
    "        self.reduced_data: Optional[np.ndarray] = None\n",
    "        self.labels: Optional[np.ndarray] = None\n",
    "        self.stats: Dict[str, Any] = {}\n",
    "        self.pca: Optional[PCA] = None\n",
    "        self.clusterer: Optional[Any] = None\n",
    "    \n",
    "    def apply_pca(self, n_components: int = 30) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Apply PCA dimensionality reduction.\n",
    "        \n",
    "        Args:\n",
    "            n_components: Number of principal components\n",
    "            \n",
    "        Returns:\n",
    "            Reduced data matrix\n",
    "        \"\"\"\n",
    "        print(f\"\\nüîß Applying PCA: {self.vector_matrix.shape[1]} ‚Üí {n_components} dimensions\")\n",
    "        \n",
    "        self.pca = PCA(n_components=n_components)\n",
    "        self.reduced_data = self.pca.fit_transform(self.vector_matrix)\n",
    "        \n",
    "        explained_var = self.pca.explained_variance_ratio_.sum()\n",
    "        print(f\"   - Explained variance: {explained_var:.2%}\")\n",
    "        \n",
    "        return self.reduced_data\n",
    "    \n",
    "    def fit(\n",
    "        self,\n",
    "        n_pca_components: int = 30,\n",
    "        min_cluster_size: int = 2,\n",
    "        min_samples: int = 1,\n",
    "        metric: str = 'euclidean',\n",
    "        cluster_selection_epsilon: float = 0.0\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Perform HDBSCAN clustering with PCA.\n",
    "        \n",
    "        Returns:\n",
    "            Array of cluster labels (-1 = noise)\n",
    "        \"\"\"\n",
    "        if not HDBSCAN_AVAILABLE:\n",
    "            raise ImportError(\"hdbscan package not installed. Run: pip install hdbscan\")\n",
    "        \n",
    "        # Apply PCA\n",
    "        self.apply_pca(n_pca_components)\n",
    "        \n",
    "        # Run HDBSCAN\n",
    "        print(f\"\\nüîß Running HDBSCAN clustering...\")\n",
    "        print(f\"   - Min cluster size: {min_cluster_size}\")\n",
    "        print(f\"   - Min samples: {min_samples}\")\n",
    "        print(f\"   - Metric: {metric}\")\n",
    "        \n",
    "        self.clusterer = hdbscan.HDBSCAN(\n",
    "            min_cluster_size=min_cluster_size,\n",
    "            min_samples=min_samples,\n",
    "            metric=metric,\n",
    "            cluster_selection_epsilon=cluster_selection_epsilon\n",
    "        )\n",
    "        self.labels = self.clusterer.fit_predict(self.reduced_data)\n",
    "        \n",
    "        self._calculate_stats()\n",
    "        self._print_summary()\n",
    "        \n",
    "        return self.labels\n",
    "    \n",
    "    def _calculate_stats(self) -> None:\n",
    "        \"\"\"Calculate clustering statistics.\"\"\"\n",
    "        labels = self.labels\n",
    "        \n",
    "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "        n_noise = (labels == -1).sum()\n",
    "        n_clustered = len(labels) - n_noise\n",
    "        \n",
    "        # Silhouette score on reduced data\n",
    "        silhouette = np.nan\n",
    "        if 1 < n_clusters < len(labels):\n",
    "            try:\n",
    "                silhouette = silhouette_score(self.reduced_data, labels)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        self.stats = {\n",
    "            'n_clusters': n_clusters,\n",
    "            'n_noise': n_noise,\n",
    "            'n_clustered': n_clustered,\n",
    "            'noise_ratio': n_noise / len(labels),\n",
    "            'avg_cluster_size': n_clustered / n_clusters if n_clusters > 0 else 0,\n",
    "            'silhouette_score': silhouette,\n",
    "            'pca_explained_variance': self.pca.explained_variance_ratio_.sum() if self.pca else None\n",
    "        }\n",
    "    \n",
    "    def _print_summary(self) -> None:\n",
    "        \"\"\"Print clustering summary.\"\"\"\n",
    "        s = self.stats\n",
    "        \n",
    "        print(f\"\\nüìä HDBSCAN Results:\")\n",
    "        print(f\"   ‚îú‚îÄ Clusters: {s['n_clusters']}\")\n",
    "        print(f\"   ‚îú‚îÄ Noise points: {s['n_noise']} ({s['noise_ratio']:.1%})\")\n",
    "        print(f\"   ‚îú‚îÄ Avg cluster size: {s['avg_cluster_size']:.1f}\")\n",
    "        if not np.isnan(s['silhouette_score']):\n",
    "            print(f\"   ‚îî‚îÄ Silhouette score: {s['silhouette_score']:.3f}\")\n",
    "        else:\n",
    "            print(f\"   ‚îî‚îÄ Silhouette score: N/A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run HDBSCAN with PCA\n",
    "if HDBSCAN_AVAILABLE:\n",
    "    hdbscan_clusterer = HDBSCANClusterer(loader.get_vector_matrix())\n",
    "    \n",
    "    hdbscan_labels = hdbscan_clusterer.fit(\n",
    "        n_pca_components=config.n_pca_components,\n",
    "        min_cluster_size=config.hdbscan_min_cluster_size,\n",
    "        min_samples=config.hdbscan_min_samples,\n",
    "        metric=config.hdbscan_metric\n",
    "    )\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è HDBSCAN not available. Skipping this section.\")\n",
    "    hdbscan_labels = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Results Export\n",
    "\n",
    "Export clustering results to Excel files for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResultsExporter:\n",
    "    \"\"\"Export clustering results to various formats.\"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame, config: ClusteringConfig):\n",
    "        self.df = df.copy()\n",
    "        self.config = config\n",
    "    \n",
    "    def export(\n",
    "        self,\n",
    "        labels: np.ndarray,\n",
    "        output_path: Optional[str] = None,\n",
    "        method_name: str = \"clustering\"\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Export clustering results to Excel.\n",
    "        \n",
    "        Args:\n",
    "            labels: Cluster labels array\n",
    "            output_path: Output file path (auto-generated if None)\n",
    "            method_name: Name of clustering method for filename\n",
    "            \n",
    "        Returns:\n",
    "            Path to exported file\n",
    "        \"\"\"\n",
    "        if output_path is None:\n",
    "            output_path = f\"{method_name}_results.xlsx\"\n",
    "        \n",
    "        # Assign labels to valid vectors\n",
    "        valid_mask = self.df['vector_array'].apply(len) == self.config.vector_dimension\n",
    "        self.df.loc[valid_mask, 'cluster'] = labels\n",
    "        \n",
    "        # Sort by cluster\n",
    "        export_df = self.df.sort_values(by='cluster')\n",
    "        \n",
    "        # Select columns for export\n",
    "        export_columns = [self.config.name_column, 'cluster'] if self.config.name_column in export_df.columns else ['cluster']\n",
    "        export_df = export_df[export_columns]\n",
    "        \n",
    "        # Export to Excel\n",
    "        export_df.to_excel(output_path, index=False)\n",
    "        \n",
    "        print(f\"\\nüíæ Results exported to: {output_path}\")\n",
    "        print(f\"   - Total rows: {len(export_df):,}\")\n",
    "        \n",
    "        return output_path\n",
    "    \n",
    "    def export_both(\n",
    "        self,\n",
    "        dbscan_labels: np.ndarray,\n",
    "        hdbscan_labels: Optional[np.ndarray] = None\n",
    "    ) -> None:\n",
    "        \"\"\"Export results from both clustering methods.\"\"\"\n",
    "        \n",
    "        # DBSCAN results\n",
    "        self.export(dbscan_labels, \"dbscan_results.xlsx\", \"dbscan\")\n",
    "        \n",
    "        # HDBSCAN results (if available)\n",
    "        if hdbscan_labels is not None:\n",
    "            self.export(hdbscan_labels, \"hdbscan_results.xlsx\", \"hdbscan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results\n",
    "exporter = ResultsExporter(df, config)\n",
    "\n",
    "# Export DBSCAN results\n",
    "exporter.export(dbscan_labels, config.output_xlsx_path, \"dbscan\")\n",
    "\n",
    "# Export HDBSCAN results if available\n",
    "if hdbscan_labels is not None:\n",
    "    exporter.export(hdbscan_labels, \"hdbscan_results.xlsx\", \"hdbscan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Visualization\n",
    "\n",
    "Visualize clustering results using PCA-reduced dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusterVisualizer:\n",
    "    \"\"\"Visualize clustering results.\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_matrix: np.ndarray):\n",
    "        self.vector_matrix = vector_matrix\n",
    "        self.pca_2d: Optional[np.ndarray] = None\n",
    "    \n",
    "    def _get_2d_projection(self) -> np.ndarray:\n",
    "        \"\"\"Get 2D PCA projection for visualization.\"\"\"\n",
    "        if self.pca_2d is None:\n",
    "            pca = PCA(n_components=2)\n",
    "            self.pca_2d = pca.fit_transform(self.vector_matrix)\n",
    "        return self.pca_2d\n",
    "    \n",
    "    def plot_clusters(\n",
    "        self,\n",
    "        labels: np.ndarray,\n",
    "        title: str = \"Cluster Visualization\",\n",
    "        figsize: Tuple[int, int] = (12, 8)\n",
    "    ) -> None:\n",
    "        \"\"\"Plot clusters in 2D PCA space.\"\"\"\n",
    "        projection = self._get_2d_projection()\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        \n",
    "        # Plot noise points first (in gray)\n",
    "        noise_mask = labels == -1\n",
    "        if noise_mask.any():\n",
    "            ax.scatter(\n",
    "                projection[noise_mask, 0],\n",
    "                projection[noise_mask, 1],\n",
    "                c='lightgray',\n",
    "                alpha=0.5,\n",
    "                s=20,\n",
    "                label='Noise'\n",
    "            )\n",
    "        \n",
    "        # Plot clustered points\n",
    "        clustered_mask = ~noise_mask\n",
    "        if clustered_mask.any():\n",
    "            scatter = ax.scatter(\n",
    "                projection[clustered_mask, 0],\n",
    "                projection[clustered_mask, 1],\n",
    "                c=labels[clustered_mask],\n",
    "                cmap='tab20',\n",
    "                alpha=0.7,\n",
    "                s=30\n",
    "            )\n",
    "            plt.colorbar(scatter, ax=ax, label='Cluster ID')\n",
    "        \n",
    "        ax.set_xlabel('PCA Component 1')\n",
    "        ax.set_ylabel('PCA Component 2')\n",
    "        ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "        ax.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_cluster_sizes(\n",
    "        self,\n",
    "        labels: np.ndarray,\n",
    "        title: str = \"Cluster Size Distribution\",\n",
    "        top_n: int = 20\n",
    "    ) -> None:\n",
    "        \"\"\"Plot cluster size distribution.\"\"\"\n",
    "        sizes = pd.Series(labels).value_counts().sort_values(ascending=True)\n",
    "        \n",
    "        # Separate noise from clusters\n",
    "        noise_size = sizes.get(-1, 0)\n",
    "        cluster_sizes = sizes.drop(-1, errors='ignore').tail(top_n)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        \n",
    "        colors = ['coral' if x == -1 else 'steelblue' for x in cluster_sizes.index]\n",
    "        \n",
    "        cluster_sizes.plot(kind='barh', ax=ax, color='steelblue')\n",
    "        \n",
    "        ax.set_xlabel('Number of Points')\n",
    "        ax.set_ylabel('Cluster ID')\n",
    "        ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # Add noise annotation\n",
    "        if noise_size > 0:\n",
    "            ax.annotate(\n",
    "                f'Noise points: {noise_size}',\n",
    "                xy=(0.95, 0.05),\n",
    "                xycoords='axes fraction',\n",
    "                ha='right',\n",
    "                fontsize=10,\n",
    "                color='coral'\n",
    "            )\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def compare_methods(\n",
    "        self,\n",
    "        dbscan_labels: np.ndarray,\n",
    "        hdbscan_labels: Optional[np.ndarray] = None\n",
    "    ) -> None:\n",
    "        \"\"\"Compare clustering results from different methods.\"\"\"\n",
    "        projection = self._get_2d_projection()\n",
    "        \n",
    "        n_methods = 2 if hdbscan_labels is not None else 1\n",
    "        fig, axes = plt.subplots(1, n_methods, figsize=(6 * n_methods, 5))\n",
    "        \n",
    "        if n_methods == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        # DBSCAN plot\n",
    "        ax = axes[0]\n",
    "        scatter = ax.scatter(\n",
    "            projection[:, 0],\n",
    "            projection[:, 1],\n",
    "            c=dbscan_labels,\n",
    "            cmap='tab20',\n",
    "            alpha=0.7,\n",
    "            s=20\n",
    "        )\n",
    "        ax.set_title(f'DBSCAN\\n({len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)} clusters)')\n",
    "        ax.set_xlabel('PCA 1')\n",
    "        ax.set_ylabel('PCA 2')\n",
    "        \n",
    "        # HDBSCAN plot\n",
    "        if hdbscan_labels is not None:\n",
    "            ax = axes[1]\n",
    "            scatter = ax.scatter(\n",
    "                projection[:, 0],\n",
    "                projection[:, 1],\n",
    "                c=hdbscan_labels,\n",
    "                cmap='tab20',\n",
    "                alpha=0.7,\n",
    "                s=20\n",
    "            )\n",
    "            ax.set_title(f'HDBSCAN + PCA\\n({len(set(hdbscan_labels)) - (1 if -1 in hdbscan_labels else 0)} clusters)')\n",
    "            ax.set_xlabel('PCA 1')\n",
    "            ax.set_ylabel('PCA 2')\n",
    "        \n",
    "        plt.suptitle('Clustering Method Comparison', fontsize=14, fontweight='bold', y=1.02)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "visualizer = ClusterVisualizer(loader.get_vector_matrix())\n",
    "\n",
    "# Plot DBSCAN clusters\n",
    "visualizer.plot_clusters(dbscan_labels, title=\"DBSCAN Clustering Results\")\n",
    "\n",
    "# Plot cluster size distribution\n",
    "visualizer.plot_cluster_sizes(dbscan_labels, title=\"DBSCAN Cluster Size Distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare methods if HDBSCAN was run\n",
    "if hdbscan_labels is not None:\n",
    "    visualizer.compare_methods(dbscan_labels, hdbscan_labels)\n",
    "    visualizer.plot_cluster_sizes(hdbscan_labels, title=\"HDBSCAN Cluster Size Distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ Quick Reference\n",
    "\n",
    "### Choosing Parameters\n",
    "\n",
    "| Parameter | Higher Value | Lower Value |\n",
    "|-----------|-------------|-------------|\n",
    "| **similarity_threshold** | Tighter clusters, more noise | Looser clusters, less noise |\n",
    "| **min_samples** | More robust clusters, more noise | More clusters, potentially less meaningful |\n",
    "| **n_pca_components** | More information preserved | Faster computation, more noise reduction |\n",
    "\n",
    "### When to Use Each Method\n",
    "\n",
    "| Method | Best For |\n",
    "|--------|----------|\n",
    "| **DBSCAN** | High-dimensional embeddings with clear similarity structure |\n",
    "| **HDBSCAN + PCA** | Very large datasets, noisy data, varying cluster densities |\n",
    "\n",
    "### Interpreting Results\n",
    "\n",
    "- **Silhouette Score**: Higher is better (range: -1 to 1)\n",
    "  - \\> 0.5: Strong structure\n",
    "  - 0.25-0.5: Reasonable structure\n",
    "  - < 0.25: Weak or no structure\n",
    "  \n",
    "- **Noise Ratio**: \n",
    "  - < 10%: Very clean clustering\n",
    "  - 10-30%: Normal for real-world data\n",
    "  - \\> 30%: Consider adjusting parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìù Notes\n",
    "\n",
    "Use this cell to record your analysis notes and findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your notes here\n",
    "# - \n",
    "# - "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
